{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK45WNYajXNy"
   },
   "source": [
    "**Curso de Inteligencia Artificial y Aprendizaje Profundo**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-AhVYeBWgQ3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# !pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Tz81NDpjXN6"
   },
   "source": [
    "## Introducción\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBAPCkGHjXN6"
   },
   "source": [
    "Gran conjunto de datos basado en sonetos de Shekespeare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEoSH791jXN6"
   },
   "source": [
    "## Librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# diccionarios especiales para puntuación y palabras vacias\n",
    "nltk.download('punkt') # Manejo de puntuación\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lematizador basado en WordNet de nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# steemer de nltk. Raiz de las palabras\n",
    "#from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUchZM1SjXN7"
   },
   "source": [
    "## Lee los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bW35Wh_Wjjik"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHWEJnZojrf4"
   },
   "outputs": [],
   "source": [
    "ruta = 'gdrive/My Drive/Colab Notebooks/Reto 5/cuentos.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AwZXvRdjXN8"
   },
   "outputs": [],
   "source": [
    "data = open(ruta, encoding=\"utf8\").read()\n",
    "data = data.replace(',', '')\n",
    "data = data.replace('«', '')\n",
    "data = data.replace('»', '')\n",
    "corpus = data.lower().split(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqmOd2H7w0X1"
   },
   "outputs": [],
   "source": [
    "#nombre_gpu = tf.test.gpu_device_name()\n",
    "#if nombre_gpu != '/device:GPU:0':\n",
    "#  raise SystemError('GPU no encontrada')\n",
    "#print('GPU encontrada: {}'.format(nombre_gpu))\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itb-CsxKjXN8"
   },
   "source": [
    "## Tokeniza el texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jspjwx7VDA4F"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#print('Dictionary: ',tokenizer.word_index,'\\n')\n",
    "print('index de verdad = ',tokenizer.word_index['verdad'])\n",
    "print('\\nTotal de palabras en le Tokenizer (+OOV) =',total_words,'\\n')\n",
    "print('Primeras Lineas del corpus: \\n')\n",
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3k-HNRHjXN9"
   },
   "source": [
    "## Crea n-Grams para datos de entrenamiento y etiquetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhZcXVeejXN-"
   },
   "outputs": [],
   "source": [
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "print('input_sequences[0:10] = ',input_sequences[:10],'\\n')\n",
    "\n",
    "# Determine longest n-gram\n",
    "max_seq_test_len = max([len(x) for x in input_sequences])\n",
    "print('\\nlong. n-grama más largo:',max_seq_test_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHzB8t8UjXN-"
   },
   "outputs": [],
   "source": [
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omyaBjyiF7Ug"
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7SadaIFjXN_"
   },
   "source": [
    "## Crea el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 50, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1oEl6lJjXN_"
   },
   "outputs": [],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv_MVsWVjXOA"
   },
   "source": [
    "## Entrena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja_5TU3EN4Af"
   },
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('gdrive/My Drive/Colab Notebooks/Reto 5/TextGeneratorModel2.h5')\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intente correr este modelo como unas 15 veces y nunca pude lograr el no se me rompiera cuando faltaba un epochs por correr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIg2f1HBxqof"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def entrenamiento_gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    model.fit(predictors, label, epochs=20, verbose=1)\n",
    "  return None\n",
    "\n",
    "gpu_time = timeit.timeit('entrenamiento_gpu()', number=1, setup='from __main__ import entrenamiento_gpu')\n",
    "\n",
    "#history = model.fit(predictors, label, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBM72My-L3V1"
   },
   "outputs": [],
   "source": [
    "model.save('gdrive/My Drive/Colab Notebooks/Reto 5/TextGeneratorModel3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fXTEO3GJ282"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = model.history.history['accuracy']\n",
    "loss = model.history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IbCXbwOjXOB"
   },
   "source": [
    "## Genera texto automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [],
   "source": [
    "seed_text = \"Hoy me levante pensando\"\n",
    "next_words = 50\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "uNEGHJj8jXOC"
   },
   "source": [
    "¿Quién eres, tan cruel? en ti tienes libre, ten cuidado, muéstrame el cielo muerto, consideró que yacía en ella mucho tiempo solo, no debes esforzarte por escribir el tono más amplio, todavía verdadero, palabras de orgullo, he aquí, viejo odio, parte vieja, mi punto de vista, eres orgullo, demuéstrame 'no hay nada nuevo. vale la pena y tu ternura te hizo escribirte brillante 'ninguna parte nueva yo joven yo viejo escribe para mí amo no pensé que tu vista debe ser nueva debe encontrar rasgados claros teñidos nuevos pechos falta dónde cómo estás ahora jurando escribe viejo amor demuéstrame escribe antiguo para ti en ti"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "GeneradorDeTexto_AG.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
